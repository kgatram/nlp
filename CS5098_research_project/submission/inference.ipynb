{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebbf23-2a6c-420b-886b-c88f409ae33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "model_name = '224_model' # please enter 224_model or 32_model\n",
    "op_size = 224 # please enter 224 or 32\n",
    "input_image_path = 'test_input.jpg'\n",
    "# target can be left when inference is required without target and execute the cell named Infer without target image\n",
    "target_image_path = 'test_target.png' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f472fa7-7364-4390-b50a-04c8b964225b",
   "metadata": {},
   "source": [
    "## Run this cell in all the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eadf3e-0a56-46f1-8812-8f588fa4b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "from math import exp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# loading the distilled small visual transformer\n",
    "dino_s = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# loading the pickle file of the scaler object which is fitted with the training data\n",
    "try:\n",
    "    with open('scaler.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "except IOError:\n",
    "    print('error')\n",
    "\n",
    "# defining the Scaled scale invariant loss\n",
    "def ssim_loss(depth_pred, depth_gt):\n",
    "    depth_gt_np = depth_gt.detach().numpy()\n",
    "    depth_pred_np = depth_pred.detach().numpy()\n",
    "    ssim_loss = (1.0 - ssim(depth_gt_np, depth_pred_np, data_range = depth_pred_np.max() - depth_pred_np.min()))\n",
    "    return ssim_loss\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mseLoss = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, depth_pred, depth_gt):\n",
    "        mse_loss = self.mseLoss(depth_pred, depth_gt)\n",
    "        ssim_loss_value = torch.tensor(ssim_loss(depth_pred, depth_gt), dtype=torch.float32)\n",
    "\n",
    "        total_loss = mse_loss * exp(ssim_loss_value)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# plotting the tensor into an image\n",
    "def plot_the_tensor(tns):\n",
    "    tns_np = tns.detach().cpu().numpy().astype('uint8')\n",
    "    tns_img = Image.fromarray(tns_np)\n",
    "    plt.imshow(tns_img)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()\n",
    "\n",
    "# converting the pixel estimated values to corresponding depth values\n",
    "def depth_conversion(pred_pixel_values):\n",
    "    max_pixel_value = 255\n",
    "    max_depth_value = 5.4\n",
    "    pred_depth_values = (pred_pixel_values * max_depth_value) / max_pixel_value\n",
    "    return pred_depth_values\n",
    "\n",
    "# defining the feed-forward network\n",
    "class DepthEstimationModel(nn.Module):\n",
    "    def __init__(self, op_size):\n",
    "        super(DepthEstimationModel, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(257*384, 1024*2),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(1024*2, 1024*2),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(1024*2, op_size*op_size),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)  # Passing through the fully connected layers\n",
    "        x = x.view(x.size(0), op_size, op_size)\n",
    "        return x\n",
    "\n",
    "# creating an instance of the feed-forward network and loading the state_dict\n",
    "model = DepthEstimationModel(op_size)\n",
    "model.load_state_dict(torch.load(f'{model_name}.pt'))\n",
    "# .eval() ensures that the weights are locked and not going to update\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a32f13-465c-4141-94c1-3c68b9cac55f",
   "metadata": {},
   "source": [
    "## Execute the below cell to infer without target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013010d6-c3b0-4a07-893e-bbefdec70eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3\n",
    "def infer_without_target(image_path):\n",
    "    start = time.time()\n",
    "    rgb_image = Image.open(image_path).resize((224,224))\n",
    "    rgb = transform(rgb_image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        dino_features = dino_s.forward_features(rgb)\n",
    "\n",
    "    patch_tokens = dino_features['x_norm_patchtokens']\n",
    "    cls_tokens = dino_features['x_norm_clstoken']\n",
    "\n",
    "    # concatinating the cls and patch tokens\n",
    "    concat = torch.cat((cls_tokens.unsqueeze(0), patch_tokens),dim=1).squeeze(0)        \n",
    "\n",
    "    # normalising the features\n",
    "    concat_norm = scaler.transform(concat)\n",
    "    # converting the features to a tensor\n",
    "    concat_norm = torch.tensor(concat_norm,dtype=torch.float32).unsqueeze(0)\n",
    "    # feeding the fetures to the network for inference    \n",
    "    predicted_depth = model(concat_norm).squeeze(0)\n",
    "    end = time.time()\n",
    "    total = end - start\n",
    "    total\n",
    "    print('Time taken for inference is: ',total)\n",
    "    print('Predicted pixel values: ',predicted_depth)\n",
    "    print('Predicted depth estimates: ',depth_conversion(predicted_depth))\n",
    "    plot_the_tensor(predicted_depth)\n",
    "    display(rgb_image)\n",
    "\n",
    "infer_without_target(input_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706d56b-3025-45c0-9165-21d0f2350baa",
   "metadata": {},
   "source": [
    "## Execute the below cell to infer with target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787b937-aa1b-4327-9d3d-1941af90f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4\n",
    "def infer_with_target(rgb_path, target_path, op_size):\n",
    "    criterion = CustomLoss()\n",
    "    start = time.time()\n",
    "\n",
    "    rgb_image = Image.open(rgb_path).resize((224,224))\n",
    "    gt_depth_image = Image.open(target_path).convert('L').resize((op_size,op_size))\n",
    "        \n",
    "    rgb = transform(rgb_image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dino_features = dino_s.forward_features(rgb)\n",
    "\n",
    "    patch_tokens = dino_features['x_norm_patchtokens']\n",
    "    cls_tokens = dino_features['x_norm_clstoken']\n",
    "    concat = torch.cat((cls_tokens.unsqueeze(0), patch_tokens),dim=1).squeeze(0)        \n",
    "\n",
    "    concat_norm = scaler.transform(concat)\n",
    "            \n",
    "    concat_norm = torch.tensor(concat_norm,dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "    predicted_depth = model(concat_norm).squeeze(0)\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "        \n",
    "    gt_np = np.array(gt_depth_image).astype(np.float32)\n",
    "    gt_tensor = torch.tensor(gt_np)\n",
    "\n",
    "    print(predicted_depth.shape)\n",
    "    print('predicted pixel values: ',predicted_depth.detach().numpy().astype('uint8'))\n",
    "    print('Predicted depth estimates: ',depth_conversion(predicted_depth))\n",
    "    print('ground truth: ',gt_tensor.detach().numpy().astype('uint8'))\n",
    "\n",
    "    loss = criterion(predicted_depth, gt_tensor)\n",
    "\n",
    "    print('Time taken: ',total_time)\n",
    "    print('Loss: ',loss)\n",
    "    print('Predicted depth image:')\n",
    "    plot_the_tensor(predicted_depth)\n",
    "    print('Ground-truth depth:')\n",
    "    plot_the_tensor(gt_tensor)\n",
    "    print('Input image:')\n",
    "    display(rgb_image)\n",
    "\n",
    "\n",
    "infer_with_target(input_image_path, target_image_path, op_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a957bf-d479-4ae1-bde1-44c0b8195d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
